# HH_SSAR
**HeadHunter Semantic Similarity Analysis for Resume**  
*Анализ семантического соответствия резюме и вакансий на HH.ru с обучением Word2Vec на русской Википедии.

## Возможности
#### 1. Модуль HH
Сбор данных с HH.ru:
   - Сбор и обработка вакансий через API
   - Парсинг резюме со страницы поиска или по прямым ссылкам
   - Сохранение/загрузка в CSV
#### 2. Модуль Word2Vec
Обучение векторных представлений:
   - Загрузка и обработка дампов статей с Википедии
   - Обучение моделей Word2Vec с сохранением эпох и возможностью продолжить позже
   - Сохранение/загрузка моделей
#### 3. Модуль UltimateMatchingModel
Сопоставление резюме с вакансиями:
   - Семантическое сходство по Word2Vec
   - Многокритериальная оценка (навыки, опыт, зарплата, месторасположение, образование)
   - Использует взвешенную систему оценок (семантика: 35%, критерии: 25%)
   - Вычисляет итоговый score и уровень уверенности в результате
   - Ранжирование и сводка результатов

## Типичный сценарий
1) **Запустить SSAR.bat**
2) **Собрать данные:**
   - HH → Скачать вакансии с HH.ru → Сохранить в CSV
   - HH → Скачать резюме по ссылке → Сохранить в CSV
3) **!! НЕОБЯЗАТЕЛЬНЫЙ ПУНКТ - Обучить модель:**
   - Word2Vec → Обучить модель (займет 2-4 часа)
4) **Выполнить анализ:**
   - UltimateMatcher → Выполнить сопоставление 
                     → Выбрать модель word2Vec 
                     → Выбрать файлы с резюме и вакансиями 
                     → Просмотреть результаты

## Полезно знать
- Данные о вакансиях и резюме сохраняется в ./data/hh/
- Кэш предложений из дампов Википедии сохраняется в ./data/cache/wiki_clean_cache_data.txt
- Эпохи сохраняются в ./word2vec_epochs/
- Итоговая модель сохраняется в ./models/
- Параметры модели по умолчанию: vector_size=300, window=2, min_count=10


## Пошаговый процесс работы
Система состоит из трех модулей и запускается через SSAR.bat, который находит Python, создает .venv, ставит зависимости и открывает интерактивное меню (main.py).

```bash
# Способ 1: Двойной клик по файлу в проводнике Windows
SSAR.bat

# Способ 2: Из командной строки
cd путь\HH_SSAR
SSAR.bat
```

После успешного запуска SSAR.bat откроется главное меню с выбором модулей:
```
Пожалуйста, выберите модуль:
> HH
  Word2Vec
  UltimateMatcher
  Выход
```

### Этап 1: Модуль HH - Сбор и управление данными
#### 1.1 Меню модуля HH
```
Пожалуйста, выберите опцию:
> Скачать вакансии с HH.ru       # API загрузка вакансий
  Загрузить вакансии из файла    # Чтение существующих CSV
  Скачать резюме с HH.ru         # Парсинг открытых резюме
  Скачать резюме по ссылкам      # По прямым URL (только открытые резюме)
  Загрузить резюме из файла      # Чтение существующих CSV
  Выход                          # Возврат в главное меню
```

#### 1.2 Сбор вакансий с HeadHunter (рекомендуется начать с этого)

**Шаг 1: Выбор региона**
```
> Скачать вакансии с HH.ru

Получаем список доступных регионов...
Список доступных регионов:
> 1: Москва
  2: Санкт-Петербург
  Exit
```

**Объяснение:** Система получает актуальный список регионов через HeadHunter API (`https://api.hh.ru/areas`), фильтрует только Москву и СПб для оптимизации.

**Шаг 2: Поисковый запрос**
```
Введите текст для поиска вакансий (по умолчанию "Data Science"): 
```

**Примеры запросов:**
- `Python разработчик` - для Python-разработчиков
- `Data Science` - для специалистов по данным
- `Frontend разработчик` - для фронтенд-разработчиков
- `Machine Learning` - для ML-инженеров

**Шаг 3: Процесс сбора данных**
```
Найдено вакансий: 1247
Страниц с вакансиями: 13
Ссылка для просмотра: https://hh.ru/search/vacancy?area=1&text=Python%20разработчик

Извлечение данных о вакансиях: 100%|████████| 1247/1247 [21:03<00:00, 1.02vacancy/s]
```

**Что происходит технически:**
- Запрос к API: `https://api.hh.ru/vacancies?area=1&only_with_salary=true&text=Python%20разработчик&per_page=100`
- Обход всех страниц результатов (по 100 вакансий на страницу)
- Пауза 3 секунды между запросами (защита от блокировки)
- Извлечение информации по каждой вакансии
- Парсинг JSON и создание объектов `VacancyData`

**Шаг 4: Сохранение результатов**
```
Сохранить данные в файл? (y/n): y
Данные сохранены в файл: ./data/hh/hh_vacancies_Python_разработчик_Москва_20241220_143502.csv
```

**Структура сохраняемых данных:**
- `vacancy_id` - ID вакансии
- `name` - Название должности  
- `area` - Регион
- `salary_from/salary_to` - Диапазон зарплаты
- `experience_id` - Требуемый опыт
- `key_skills` - Ключевые навыки
- `employer_name` - Название компании
- `description` - Полное описание (очищенное от HTML)
- `url` - Ссылка на вакансию

#### 1.3 Сбор резюме с HeadHunter

**Вариант A: Скачать резюме с HH.ru (поиск по региону)**

```
> Скачать резюме с HH.ru

Получаем список доступных регионов...
Список доступных регионов:
> 1: Москва
  2: Санкт-Петербург
  
Введите текст для поиска резюме: Python разработчик
```

**Ограничения:** Доступны только публичные резюме в результатах поиска.

**Вариант B: Скачать резюме по ссылкам (рекомендуется)**

```
> Скачать резюме по ссылкам

Введите ссылку на резюме (или несколько через запятую): 
https://hh.ru/resume/e0030b08ff0ccd25890039ed1f4d706b6f636e, https://hh.ru/resume/another_id
```

**Процесс обработки:**
```
Извлечение данных о резюме: 100%|████████| 2/2 [00:08<00:00, 4.21resume/s]
Резюме успешно обработано!

=== ПРЕДПРОСМОТР ДАННЫХ ===
Размер DataFrame: 2 строки, 11 столбцов

--- Первые 5 записей ---
  resume_id    name                area     salary  skills
0 e0030b08ff   Python разработчик  Москва   180000  Python, Django, PostgreSQL
1 another_id   Data Scientist      СПб      200000  Python, ML, Pandas
```

**Что извлекается из резюме:**
- Парсинг HTML-страницы резюме
- Извлечение названия позиции (`data-qa="resume-block-title-position"`)
- Определение местоположения (`data-qa="resume-personal-address"`)
- Парсинг зарплатных ожиданий с определением валюты
- Анализ опыта работы и сопоставление с справочником HH
- Извлечение навыков (`data-qa="resume-block-skills-element"`)
- Сбор описания опыта работы (`data-qa="resume-block-experience-description"`)
- Извлечение информации об образовании

#### 1.4 Загрузка данных из файлов

**Для вакансий:**
```
> Загрузить вакансии из файла

Выберите файл с данными:
> hh_vacancies_Python_разработчик_Москва_20241220_143502.csv
  hh_vacancies_Data_Science_СПб_20241219_120301.csv
  hh_vacancies_Frontend_Москва_20241218_095430.csv
```

**Для резюме:**
```
> Загрузить резюме из файла

Выберите файл с данными:
> hh_resumes_Python_разработчик_variously_id_20241220_145601.csv
  hh_resumes_Data_Science_Москва_20241219_134502.csv
```

**Техническая реализация:**
- Сканирование папки `./data/hh/` с фильтром по типу данных
- Чтение CSV с разделителем `;` и кодировкой `utf-8-sig`
- Создание DataFrame и проверка структуры данных
- Вывод статистики загруженных данных

---

### Этап 2: Модуль Word2Vec - Обучение семантических моделей

#### 2.1 Меню модуля Word2Vec

**При первом запуске (модель не загружена):**
```
Пожалуйста, выберите опцию:
> Обучить модель     # Обучение с нуля или продолжение
  Загрузить модель   # Загрузка существующей модели
  Выход
```

**После загрузки модели:**
```
Пожалуйста, выберите опцию:
> Обучить модель      # Дообучение или новая модель
  Тестировать модель  # Проверка качества модели
  Выход
```

#### 2.2 Обучение новой модели (первый запуск)

**Шаг 1: Проверка кэшированных данных**
```
> Обучить модель

Generate 'train_data' from ./data/cache/wiki_clean_cache_data.txt:
> Yes  # Использовать готовый кэш (быстро)
  No   # Скачать и обработать новые данные Wikipedia
```

**Объяснение выбора:**
- **Yes**: Использует предварительно обработанные предложения из кэша (если существует)
- **No**: Скачивает дампы Wikipedia и обрабатывает их заново (2-4 часа)

**Шаг 2: Загрузка данных Wikipedia (если выбран "No")**
```
Please choose a download type:
> Full   # Полный дамп ruwiki (~4GB, лучшее качество)
  Parts  # Части дампа (~800MB каждая, быстрее)
```

**Технический процесс:**
- Запрос к `https://dumps.wikimedia.org/ruwiki/latest/`
- Поиск файлов: `ruwiki-latest-pages-articles-multistream*.xml.bz2`
- Скачивание с прогресс-баром: `wget` или `requests.iter_content`

**Шаг 3: Настройка сохранения обработанных данных**
```
Save preparing data?:
> Yes  # Сохранить в кэш для будущих запусков
  No   # Обрабатывать в памяти (не сохранять)
```

**Шаг 4: Обработка данных Wikipedia**
```
Processing file 1 of 1: ruwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2

Extracting articles from ruwiki-...: 41242 articles [15:23<00:00, 44.67 articles/s]
```

**Что происходит в процессе обработки:**
1. **Извлечение статей:** `WikiCorpus` распаковывает XML и извлекает текст
2. **Токенизация:** `WordPunctTokenizer` разбивает на слова
3. **Очистка:** Удаление знаков препинания, цифр, коротких слов
4. **Лемматизация:** `pymorphy2.MorphAnalyzer` приводит к нормальной форме
5. **Фильтрация:** Удаление слов длиной < 2 или > 20 символов

**Шаг 5: Обучение модели Word2Vec**
```
Creating Word2Vec model and train with 1 epochs...
Training Word2Vec model...: 100%|████████| 1/1 [1:23:45<00:00, 5025.32s/epoch]

Epoch 1 of 1 completed. Model saved to ./models/word2vec_epochs/word2vec_epoch_0.model
Model training completed and saved as 'wikidumps-300.model'
```

**Параметры модели по умолчанию:**
- `vector_size=300` - Размерность векторов слов
- `window=2` - Размер контекстного окна  
- `min_count=10` - Минимальная частота слова для включения в словарь
- `workers=multiprocessing.cpu_count()` - Количество процессов

**Технические детали обучения:**
- **Skip-gram архитектура** для лучшего качества редких слов
- **Hierarchical Softmax** для ускорения обучения
- **Сохранение промежуточных эпох** в `./models/word2vec_epochs/`
- **Callback система** для отслеживания прогресса

#### 2.3 Продолжение обучения существующей модели

```
Do you want to continue training from the last epoch?
> Yes  # Продолжить с последней сохраненной эпохи
  No   # Удалить старые эпохи и начать заново
```

**При выборе "Yes":**
- Поиск последней эпохи по времени создания файла
- Извлечение номера эпохи из имени файла (`word2vec_epoch_(\d+)\.model`)
- Загрузка модели: `Word2Vec.load(last_epoch_file)`
- Продолжение обучения: `model.train(...)`

#### 2.4 Загрузка готовой модели

```
> Загрузить модель

Select a model to load:
> wikidumps-300.model                   # Основная модель (300 измерений)
  wikidumps-200-vocab150k.model         # Компактная модель
  wikidumps-500-maxquality.model        # Модель высокого качества

Model [./models/wikidumps-300.model] loaded.
```

#### 2.5 Тестирование модели

```
> Тестировать модель

Введите слово, чтобы найти похожие слова (или введите '/exit' для выхода): python

Наиболее похожие слова к 'python':
    ~программирование: 0.8234
    ~разработка: 0.7891
    ~javascript: 0.7456
    ~php: 0.7123
    ~java: 0.6987
```

**Технический процесс:**
- Проверка наличия слова в словаре: `word in model.wv`
- Вычисление похожих слов: `model.wv.most_similar(word, topn=5)`
- Косинусное сходство векторов слов
- Сортировка по убыванию сходства

**Интерпретация результатов:**
- **> 0.8**: Очень высокое сходство (синонимы, тематически близкие)
- **0.6-0.8**: Высокое сходство (одна тематическая область)
- **0.4-0.6**: Среднее сходство (смежные области)
- **< 0.4**: Слабое сходство

---

### Этап 3: Модуль UltimateMatcher - Комплексный анализ соответствия

#### 3.1 Меню UltimateMatcher

```
Пожалуйста, выберите опцию:
> Выполнить сопоставление    # Запуск полного анализа
  Выход                      # Возврат в главное меню
```

#### 3.2 Инициализация компонентов системы

**Шаг 1: Загрузка модели Word2Vec**
```
> Выполнить сопоставление

Select a model to load:
> wikidumps-300.model
  wikidumps-200-vocab150k.model

Model [./models/wikidumps-300.model] loaded.
Модель Word2Vec загружена успешно
```

**Шаг 2: Выбор резюме для анализа**
```
Выберите файл с данными:
> hh_resumes_Python_разработчик_variously_id_20241220_145601.csv
  hh_resumes_Data_Science_Москва_20241219_134502.csv

Выберите данные:
> (Выбрать все)
  1: Python разработчик (Data Science)
  2: Senior Python Developer  
  3: ML Engineer
```

**Шаг 3: Выбор вакансий для сопоставления**
```
Выберите файл с данными:
> hh_vacancies_Python_разработчик_Москва_20241220_143502.csv
  hh_vacancies_Data_Science_СПб_20241219_120301.csv

Загружено 247 элементов
```

#### 3.3 Процесс анализа соответствия

```
Выполнение сопоставления резюме и вакансий...
```

**Архитектура анализа:**

**1. UltimateMatcher (главный класс)**
- Комбинирует семантический и многокритериальный анализ
- Взвешенная система оценок:
  - `semantic: 35%` - семантическое сходство
  - `multi_criteria: 25%` - критериальная оценка

**2. Word2VecMatcher (семантический анализ)**
```python
# Извлечение текста из резюме
resume_text = resume.description + " " + resume.description_about_me

# Извлечение текста из вакансии  
vacancy_text = vacancy.description

# Преобразование в векторы
resume_vector = model.text_to_vector(clean_text(resume_text))
vacancy_vector = model.text_to_vector(clean_text(vacancy_text))

# Вычисление косинусного сходства
similarity = cosine_similarity(resume_vector, vacancy_vector)
```

**3. SimplyClassificationMatcher (многокритериальный анализ)**

**Критерий 1: Совпадение навыков (35% веса)**
```python
resume_skills = {"python", "django", "postgresql", "docker"}
vacancy_skills = {"python", "flask", "mysql", "kubernetes"}

# Точные совпадения
exact_matches = resume_skills.intersection(vacancy_skills)  # {"python"}
skills_score = len(exact_matches) / len(vacancy_skills)      # 1/4 = 0.25
```

**Критерий 2: Соответствие опыта (25% веса)**
```python
experience_mapping = {
    'noExperience': 0,     # Нет опыта
    'between1And3': 1,     # 1-3 года
    'between3And6': 2,     # 3-6 лет  
    'moreThan6': 3         # Более 6 лет
}

resume_exp_level = 2    # 3-6 лет
vacancy_exp_level = 1   # 1-3 года

if resume_exp_level > vacancy_exp_level:
    score = 0.8  # Опыт больше требуемого (положительно)
elif resume_exp_level == vacancy_exp_level:
    score = 1.0  # Точное совпадение
else:
    score = 0.3  # Опыт меньше требуемого
```

**Критерий 3: Соответствие зарплаты (20% веса)**
```python
resume_salary = 180000      # Ожидания кандидата
vacancy_from = 150000       # Нижняя граница в вакансии
vacancy_to = 200000         # Верхняя граница в вакансии

if vacancy_from <= resume_salary <= vacancy_to:
    salary_score = 1.0      # В диапазоне
elif resume_salary < vacancy_from:
    # Ожидания ниже предложения (хорошо для работодателя)
    diff_ratio = (vacancy_from - resume_salary) / vacancy_from
    salary_score = max(0.6, 1.0 - diff_ratio)
else:
    # Ожидания выше предложения (плохо)
    diff_ratio = (resume_salary - vacancy_to) / vacancy_to
    salary_score = max(0.2, 1.0 - diff_ratio * 2)
```

**Критерий 4: Географическое соответствие (15% веса)**
```python
resume_area = "москва"
vacancy_area = "москва"

location_score = 1.0 if resume_area == vacancy_area else 0.3
```

**Критерий 5: Уровень образования (5% веса)**
```python
education_keywords = {
    'высшее': ['университет', 'институт', 'академия'],
    'среднее_спец': ['колледж', 'техникум'],  
    'среднее': ['школа', 'лицей', 'гимназия']
}

education_scores = {'высшее': 1.0, 'среднее_спец': 0.8, 'среднее': 0.6}
```

**4. Итоговый расчет:**
```python
# Многокритериальная оценка
multi_criteria = (
    skills_score * 0.35 +
    experience_score * 0.25 +
    salary_score * 0.20 +
    location_score * 0.15 +
    education_score * 0.05
)

# Финальная оценка
final_score = semantic_score * 0.35 + multi_criteria * 0.25

# Уровень уверенности (на основе дисперсии оценок)
confidence = max(0.1, 1.0 - np.var([semantic_score, multi_criteria]))
```

#### 3.4 Результаты анализа

```
================================================================================
РЕЗУЛЬТАТЫ СОПОСТАВЛЕНИЯ
================================================================================
Обработано резюме: 3
Всего найдено совпадений: 18
================================================================================

📄 РЕЗЮМЕ: Python разработчик (Data Science)
   ID на HeadHunter: e0030b08ff0ccd25890039ed1f4d706b6f636e
   Ссылка: https://hh.ru/resume/e0030b08ff0ccd25890039ed1f4d706b6f636e
   Найдено совпадений: 8
────────────────────────────────────────────────────────────────────────────────

   1. 💼 Python разработчик (Machine Learning)
      🏢 Компания: Яндекс
      📍 Регион: Москва  
      📊 Сходство: 87.3% (0.9) (🟢 ОТЛИЧНОЕ)
      🎯 Опыт: От 3 до 6 лет
      🔗 Ссылка: https://hh.ru/vacancy/12345678
      📝 Компоненты: semantic: 0.9, multi_criteria: 0.8

   2. 💼 Data Scientist
      🏢 Компания: Сбербанк
      📍 Регион: Москва
      📊 Сходство: 72.1% (0.8) (🟡 ХОРОШЕЕ)
      🎯 Опыт: От 1 года до 3 лет  
      🔗 Ссылка: https://hh.ru/vacancy/87654321
      📝 Компоненты: semantic: 0.7, multi_criteria: 0.7
```

#### 3.5 Интерпретация результатов

**Уровни соответствия:**
- 🟢 **ОТЛИЧНОЕ (80-100%)**: Вакансия идеально подходит кандидату
  - Высокое семантическое сходство (> 0.75)
  - Соответствие по большинству критериев
  - Рекомендуется приоритетное рассмотрение

- 🟡 **ХОРОШЕЕ (60-79%)**: Высокая вероятность подходящей позиции
  - Хорошее семантическое сходство (0.6-0.75)
  - Частичное соответствие критериев
  - Стоит рассмотреть для подачи заявки

- 🟠 **СРЕДНЕЕ (40-59%)**: Частичное соответствие
  - Среднее семантическое сходство (0.4-0.6)  
  - Требует дополнительного анализа
  - Возможно, стоит развить недостающие навыки

- 🔴 **СЛАБОЕ (<40%)**: Низкое соответствие
  - Слабое семантическое сходство (< 0.4)
  - Не рекомендуется к рассмотрению

**Анализ компонентов:**
- **semantic**: Насколько близки описания резюме и вакансии по смыслу
- **multi_criteria**: Комплексная оценка по формальным критериям

---

#### ЕСЛИ "Мало совпадений в результатах"
**Причины и решения**:
1. **Слишком узкий поиск**: Расширьте поисковые запросы
2. **Нерелевантные резюме**: Проверьте соответствие навыков в резюме
3. **Неточная модель**: Дообучите модель на большем корпусе данных

#### ЕСЛИ Недостаточно памяти при обучении
**Решения**:
1. Выберите "Parts" вместо "Full" при загрузке Wikipedia  
2. Уменьшите `vector_size` в коде (с 300 до 200 или 100)
3. Увеличьте `min_count` для уменьшения словаря
4. Закройте лишние программы